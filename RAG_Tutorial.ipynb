{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Created by: [Bharath Kumar Hemachandran](mailto:bharathh@gmail.com) [Linkedin](https://www.linkedin.com/in/bharath-hemachandran/)\n",
        "\n",
        "\n",
        "\n",
        "# **RAG - A Complete Tutorial**\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is an advanced technique that enhances the capabilities of language models by combining retrieval mechanisms with generative responses. This approach enables models to access external knowledge, improving accuracy and relevance, especially when handling complex or domain-specific queries.\n",
        "\n",
        "## **What is RAG?**\n",
        "RAG consists of two key components:\n",
        "1. **Retriever:** Finds relevant information from a knowledge base or dataset.\n",
        "2. **Generator:** Produces a response based on the retrieved information.\n",
        "\n",
        "RAG is particularly useful in scenarios where:\n",
        "- The base language model has limited knowledge.\n",
        "- Up-to-date or specific information is required.\n",
        "- Responses must be grounded in factual data.\n",
        "\n",
        "---\n",
        "\n",
        "## **What You Will Learn**\n",
        "\n",
        "By the end, you’ll have a RAG system that can:\n",
        "1. Retrieve relevant information from a dataset.\n",
        "2. Generate context-aware responses from the dataset\n",
        "\n",
        "We will be using Langchain SentenceTransformers to convert small chunks of the document into embeddings and store them into a vectorstore. We will be using FAISS for the vectorstore and use the similarity search feature to query the vectorstore.\n",
        "\n",
        "Once the query has been completed, we will then use a call to the Llama3 API hosted on [Groqcloud](https://console.groqcloud.com) to generate a context aware response.\n",
        "\n",
        "---\n",
        "\n",
        "### **How RAG Works**\n",
        "\n",
        "# **Introduction**\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is an advanced technique that enhances language models by integrating retrieval mechanisms with generative capabilities. This approach allows models to access external knowledge bases, improving accuracy and relevance, especially for complex or domain-specific queries.\n",
        "\n",
        "## **How RAG Works**\n",
        "\n",
        "[![Retrieval-Augmented Generation Workflow](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/04/25/ML-13871image003.jpg)](https://nbkomputer.com/retrieval-augmented-generation-with-langchain-amazon-sagemaker/)\n",
        "\n",
        "#####[Image Source](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/04/25/ML-13871image003.jpg)\n",
        "\n",
        "In the diagram above, the RAG process is depicted as follows:\n",
        "1. **Query Input:** A user submits a query.\n",
        "2. **Retriever:** The system searches a knowledge base to find relevant information.\n",
        "3. **Generator:** Using the retrieved information, the system generates a contextually appropriate response.\n",
        "\n",
        "This combination ensures that responses are both informed by external data and coherently generated, providing more accurate and up-to-date information.\n",
        "\n",
        "---\n",
        "\n",
        "By understanding and implementing RAG, we can develop systems that effectively leverage vast datasets to produce high-quality, context-aware responses.\n",
        "\n",
        "\n",
        "### **Application Areas**\n",
        "\n",
        "Some practical use cases of RAG include:\n",
        "- Building intelligent chatbots.\n",
        "- Creating personalized recommendation systems.\n",
        "- Answering questions with evidence-based responses.\n",
        "- Improving search engine capabilities.\n",
        "\n",
        "---\n",
        "\n",
        "With this foundation, let’s set up our environment and dive into building a RAG system!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cHfQSlYxOz8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Document Ingestion into the Document Library**\n",
        "\n",
        "In this section, we’ll outline how documents are ingested into the document library for use in a Retrieval-Augmented Generation (RAG) system. This process ensures that documents are preprocessed, embedded, and stored efficiently for retrieval during query handling.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 1: Uploading Documents**\n",
        "- Documents can be uploaded in various formats such as `.txt`, `.pdf`, `.csv`, or `.json`.\n",
        "- Use file upload options in your application or APIs to add files to the document library.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 2: Preprocessing**\n",
        "- Once uploaded, documents are preprocessed to ensure uniformity:\n",
        "  - **Text extraction**: Convert the content of files (e.g., PDFs) into plain text.\n",
        "  - **Cleaning**: Remove special characters, stopwords, or unnecessary whitespace.\n",
        "  - **Segmentation**: Split the text into smaller chunks, such as paragraphs or sentences, to facilitate efficient embedding.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 3: Embedding Generation**\n",
        "- Use a pre-trained model from **SentenceTransformers** to encode text chunks into vector embeddings.\n",
        "- Embeddings are dense numerical representations of the text, capturing its semantic meaning.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 4: Building the Index**\n",
        "- Feed the embeddings into a **FAISS** index to enable fast similarity searches.\n",
        "- Choose an appropriate FAISS index type (e.g., `IndexFlatL2` for small datasets or `IVF` for larger ones).\n",
        "- Save the index to disk for future use.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 5: Storing Metadata**\n",
        "- Alongside embeddings, store metadata (e.g., document titles, sources, or tags) to provide context during retrieval.\n",
        "- Metadata is linked to embeddings to enhance query results.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 6: Verification and Testing**\n",
        "- Validate the ingestion pipeline:\n",
        "  - Ensure all documents are correctly preprocessed.\n",
        "  - Test the FAISS index by performing similarity searches on sample queries.\n",
        "\n",
        "---\n",
        "\n",
        "Once these steps are complete, the documents are ready for retrieval in the RAG workflow, ensuring seamless integration between retrieval and generation.\n",
        "\n",
        "First, let us install and import the necessary libraries required to complete this process. Hit the run button to get started."
      ],
      "metadata": {
        "id": "EIi3-PNYTO-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain_community\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers\n",
        "!pip install huggingface-hub\n",
        "!pip install unstructured\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the missing resource"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3QhN0uNS0P0",
        "outputId": "b36148c5-d2cf-4fb8-c7fc-ba8dd67bc73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "7D2b9VzsVVb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There should be a folder called sample_data with a file called refund_policy.txt within. This contains some sample policies for a return policy for a typical ecommerce retailer. We will be using this document for our trial purposes. Hit the run button below to ensure that the document exists.\n",
        "\n",
        "If you get the following error\n",
        "```ls: cannot access 'sample_data/refund_policy.txt': No such file or directory```\n",
        "\n",
        "then on the LHS of this document, click on the folder icon, and add the following document to the sample_data folder."
      ],
      "metadata": {
        "id": "uKcG7ng4We09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the files required - first removing all the default files in the sample_data folder\n",
        "!rm sample_data/*\n",
        "!wget 'https://raw.githubusercontent.com/bharathh80/genaiworkshop/refs/heads/main/docs/refund_policy.txt' -P sample_data\n",
        "# Check if the file exists\n",
        "!ls sample_data/refund_policy.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ede5Nl-WYF5",
        "outputId": "3775a3a7-1df4-4120-9c8e-92c7953b82e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-23 07:58:26--  https://raw.githubusercontent.com/bharathh80/genaiworkshop/refs/heads/main/docs/refund_policy.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11458 (11K) [text/plain]\n",
            "Saving to: ‘sample_data/refund_policy.txt’\n",
            "\n",
            "\rrefund_policy.txt     0%[                    ]       0  --.-KB/s               \rrefund_policy.txt   100%[===================>]  11.19K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-23 07:58:26 (24.0 MB/s) - ‘sample_data/refund_policy.txt’ saved [11458/11458]\n",
            "\n",
            "sample_data/refund_policy.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify where the documents reside"
      ],
      "metadata": {
        "id": "mwY3yhDfbKcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = './sample_data'"
      ],
      "metadata": {
        "id": "MJuowF32Xe8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to load documents using the ```DirectoryLoader``` library\n",
        "\n"
      ],
      "metadata": {
        "id": "FLgEtU7wbfDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_docs(_directory):\n",
        "    loader = DirectoryLoader(_directory)\n",
        "    _documents = loader.load()\n",
        "    return _documents"
      ],
      "metadata": {
        "id": "8n-TT2QPbmUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```DirectoryLoader``` reads all files in the specified directory.\n",
        "The ```load()``` method returns a list of document objects.\n",
        "\n",
        "You can now load the documents and print their count:"
      ],
      "metadata": {
        "id": "7mAiDExNbsVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = load_docs(directory)\n",
        "print(f\"Number of documents ingested: {len(documents)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37CP9f1Jb1M8",
        "outputId": "81066fe5-d671-4a13-fb21-32340d89ffcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents ingested: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Types of Document Loaders in LangChain**\n",
        "\n",
        "LangChain offers a variety of [document loaders](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/) to import data from different sources into the standard `Document` format. Below is a summary of the different types of document loaders:\n",
        "\n",
        "| **Document Loader** | **Description** | **Use Case** |\n",
        "|---------------------|-----------------|--------------|\n",
        "| `TextLoader` | Loads simple `.txt` files as documents. | Ideal for loading plain text files. |\n",
        "| `CSVLoader` | Loads data from CSV files. | Useful for structured data stored in CSV format. |\n",
        "| `DirectoryLoader` | Loads documents from a specified directory. | Suitable for bulk loading of files from a folder. |\n",
        "| `HTMLLoader` | Loads and parses HTML files. | Appropriate for extracting text from web pages or HTML documents. |\n",
        "| `JSONLoader` | Loads data from JSON files. | Best for loading structured data in JSON format. |\n",
        "| `MarkdownLoader` | Loads and parses Markdown files. | Designed for documents written in Markdown. |\n",
        "| `UnstructuredWordDocumentLoader` | Loads Microsoft Word documents. | Useful for `.docx` files. |\n",
        "| `PyPDFLoader` | Loads and parses PDF files. | Ideal for extracting text from PDFs. |\n",
        "| `UnstructuredEmailLoader` | Loads email files. | Suitable for processing email content. |\n",
        "| `EverNoteLoader` | Loads Evernote note files. | Designed for importing notes from Evernote. |\n",
        "| `NotionDBLoader` | Loads data from Notion databases. | Useful for integrating Notion data. |\n",
        "| `ObsidianLoader` | Loads notes from Obsidian. | Best for importing markdown notes from Obsidian. |\n",
        "| `RoamLoader` | Loads data from Roam Research. | Suitable for integrating Roam Research notes. |\n",
        "| `SlackLoader` | Loads messages from Slack. | Ideal for importing Slack conversation history. |\n",
        "| `ConfluenceLoader` | Loads pages from Confluence. | Useful for integrating Confluence documentation. |\n",
        "| `GoogleDriveLoader` | Loads files from Google Drive. | Suitable for accessing documents stored in Google Drive. |\n",
        "| `S3Loader` | Loads files from AWS S3 buckets. | Best for loading data stored in S3. |\n",
        "| `WebBaseLoader` | Loads content from web pages. | Ideal for scraping and loading web content. |\n",
        "| `YouTubeLoader` | Loads transcripts from YouTube videos. | Useful for extracting text from video transcripts. |\n",
        "\n",
        "Each loader is designed to handle specific data formats and sources, enabling efficient and contextually appropriate data ingestion into LangChain.\n"
      ],
      "metadata": {
        "id": "OKyyfI7PfhgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To process the text efficiently, split it into smaller chunks using the ```RecursiveCharacterTextSplitter``` library"
      ],
      "metadata": {
        "id": "Kt4b6lRjc5je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_docs(_documents, chunk_size=1000, chunk_overlap=20):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs = text_splitter.split_documents(_documents)\n",
        "    return docs"
      ],
      "metadata": {
        "id": "28A9ttcPdHIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```chunk_size```: Maximum size of each chunk in characters.\n",
        "```chunk_overlap```: Number of overlapping characters between chunks.\n",
        "Split the loaded documents and print the number of chunks created"
      ],
      "metadata": {
        "id": "I_Zn22s4dMbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = split_docs(documents)\n",
        "print(f\"Number of chunks created: {len(chunks)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMr8Z7jXdSv0",
        "outputId": "9c055b86-d11f-43a4-c1e5-1177a0a531f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks created: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Types of Text Splitters in LangChain**\n",
        "\n",
        "LangChain provides various [text splitters](https://api.python.langchain.com/en/latest/text_splitters/index.html) to divide text into manageable chunks, each tailored for specific formats and use cases. Below is a summary of the different types of text splitters:\n",
        "\n",
        "| **Text Splitter**                          | **Description**                                                        | **Use Case**                                                                                  |\n",
        "|--------------------------------------------|------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|\n",
        "| `CharacterTextSplitter`                    | Splits text based on characters, using a specified separator.          | General-purpose splitting when simple character-based division is sufficient.               |\n",
        "| `RecursiveCharacterTextSplitter`          | Recursively splits text by characters, aiming to respect sentence boundaries and other delimiters. | Useful for maintaining semantic coherence in chunks.                                         |\n",
        "| `TokenTextSplitter`                        | Splits text into tokens using a model tokenizer.                       | Ideal for applications requiring token-level processing, such as language models.           |\n",
        "| `HTMLHeaderTextSplitter`                   | Splits HTML files based on specified header tags.                      | Suitable for processing HTML documents by sections defined by headers.                      |\n",
        "| `HTMLSectionSplitter`                     | Splits HTML files based on specified tags and font sizes.              | Useful for segmenting HTML content into meaningful sections beyond headers.                 |\n",
        "| `MarkdownHeaderTextSplitter`              | Splits Markdown files based on specified headers.                      | Designed for dividing Markdown documents into sections according to header levels.          |\n",
        "| `MarkdownTextSplitter`                    | Attempts to split text along Markdown-formatted headings.              | Effective for processing Markdown content by its structural elements.                       |\n",
        "| `NLTKTextSplitter`                        | Splits text using the NLTK package, typically at sentence boundaries.  | Appropriate for sentence-level splitting, especially in natural language processing tasks.   |\n",
        "| `PythonCodeTextSplitter`                  | Attempts to split text along Python syntax.                            | Tailored for dividing Python code into logical segments.                                    |\n",
        "| `SentenceTransformersTokenTextSplitter`   | Splits text into tokens using a sentence model tokenizer.              | Suitable for applications involving sentence embeddings and similarity tasks.               |\n",
        "| `SpacyTextSplitter`                       | Splits text using the SpaCy package, leveraging its linguistic features. | Ideal for splitting text based on linguistic components like sentences or entities.         |\n",
        "| `KonlpyTextSplitter`                      | Splits text using the Konlpy package, which is designed for Korean language processing. | Best suited for processing Korean text, utilizing Konlpy's capabilities.                    |\n",
        "| `LatexTextSplitter`                       | Attempts to split text along LaTeX-formatted layout elements.          | Useful for segmenting LaTeX documents into logical parts.                                   |\n",
        "| `RecursiveJsonSplitter`                   | Splits JSON content recursively based on specified criteria.           | Designed for breaking down JSON data structures into manageable pieces.                     |\n",
        "| `ExperimentalMarkdownSyntaxTextSplitter`  | An experimental splitter for handling Markdown syntax.                 | Useful for testing and development purposes with Markdown content.                          |\n",
        "\n",
        "Each splitter is designed to handle specific text formats and structures, enabling efficient and contextually appropriate text processing.\n"
      ],
      "metadata": {
        "id": "scwoeUZddvQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have chunked our document into small chunks, we now need to convert it into vector embeddings in order to be able to do a similarity search using FAISS.\n",
        "\n",
        "### Why is this required?\n",
        "\n",
        "The reason we create vectors using embeddings, is essential for enabling similarity search in\n",
        "\n",
        "Retrieval-Augmented Generation (RAG). Vectors represent the semantic meaning of text in a high-dimensional space, allowing the system to retrieve contextually relevant chunks of information based on a query. This step ensures that retrieval is efficient and accurate, which is critical for generating meaningful and context-aware responses.\n",
        "\n",
        "Without embeddings, the retrieval process would rely solely on keyword matching or other less effective methods, which can miss the deeper semantic connections between the query and the document content.\n",
        "\n",
        "### Alternatives to Creating Vectors for Retrieval\n",
        "\n",
        "While creating vectors is the most common and effective approach, there are some alternatives for retrieval in RAG:\n",
        "\n",
        "1. **TF-IDF (Term Frequency-Inverse Document Frequency):**\n",
        "   - This is a traditional method that uses keyword frequency to measure the importance of terms in a document.\n",
        "   - Works well for simple tasks but lacks the ability to capture semantic meaning.\n",
        "\n",
        "2. **BM25 (Best Match 25):**\n",
        "   - An advanced ranking function based on TF-IDF with better handling of term saturation and document length.\n",
        "   - Commonly used in search engines like Elasticsearch but does not encode semantic information.\n",
        "\n",
        "3. **Keyword-Based Search:**\n",
        "   - Matches exact keywords between the query and the documents.\n",
        "   - Limited to surface-level matches and not ideal for nuanced queries.\n",
        "\n",
        "4. **Ontology or Knowledge Graphs:**\n",
        "   - Uses a structured representation of knowledge to retrieve relevant information.\n",
        "   - Requires significant effort to build and maintain the ontology.\n",
        "\n",
        "5. **Rule-Based Retrieval:**\n",
        "   - Uses predefined rules or patterns for matching queries with documents.\n",
        "   - Effective in specific, controlled domains but lacks flexibility.\n",
        "\n",
        "Among these methods, vector-based retrieval using embeddings remains the most effective for RAG as it captures both syntax and semantics, enabling the retrieval of contextually relevant information for diverse and complex queries.\n"
      ],
      "metadata": {
        "id": "sDqNNumMgDAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embeddings(_chunks):\n",
        "    modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "    model_kwargs = {'device': 'cpu'}\n",
        "    encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=modelPath,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs\n",
        "    )\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "6P3-5YfEd0q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`modelPath`: Specifies the embedding model (e.g., all-MiniLM-l6-v2).\n",
        "\n",
        "`model_kwargs`: Configures the model to use a specific device (e.g., CPU).\n",
        "\n",
        "`encode_kwargs`: Provides additional encoding options\n",
        "\n",
        "Next, we create the embedding object:"
      ],
      "metadata": {
        "id": "KfAA7EXMg8hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = create_embeddings(chunks)"
      ],
      "metadata": {
        "id": "zXRN43W_hKkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our embeddings, let us create a search index using [FAISS](https://faiss.ai/) to create the document library"
      ],
      "metadata": {
        "id": "MU0rs2hqhidv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    db = FAISS.from_documents(chunks, embedding)\n",
        "    db.save_local(folder_path=\"../database/faiss_db\", index_name=\"myFaissIndex\")\n",
        "    print(\"FAISS index created\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print(\"FAISS index creation failed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-F7HGSliDGw",
        "outputId": "ff129aad-6cf9-40cd-80ad-f8b69e003e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The FAISS.from_documents() method creates the index.\n",
        "The save_local() method saves the index to disk for future use.\n",
        "\n",
        "If the previous code executed correctly, if you refresh the document folder for this Colab notebook, you will see a database folder with faiss_db created within. The folder will contain the index that can be used for later retrieval. This index can also be stored remotely for better persistence."
      ],
      "metadata": {
        "id": "54DhjO-1iPmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.load_local(folder_path=\"../database/faiss_db\", embeddings=embedding, index_name=\"myFaissIndex\", allow_dangerous_deserialization=True)\n",
        "searchDocs = db.similarity_search(\"What is the return policy for?\")\n",
        "print(searchDocs[0].page_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gUEZannDinld",
        "outputId": "ceea4dd8-4330-458c-f810-5724c7ddae3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer Responsibility: Unless otherwise stated, customers are responsible for the\n",
            "\n",
            "cost of return shipping. We recommend using a trackable shipping method and insuring\n",
            "\n",
            "the package to ensure its safe return to our facility.\n",
            "\n",
            "2.5 Exclusions:\n",
            "\n",
            "Exceptions to the Policy: AcmeCorp reserves the right to make exceptions to the return\n",
            "\n",
            "policy on a case-by-case basis. Such exceptions may include extenuating\n",
            "\n",
            "circumstances or errors on our part.\n",
            "\n",
            "2.6 International Returns:\n",
            "\n",
            "Additional Considerations: For returns originating from outside our domestic shipping\n",
            "\n",
            "region, additional restrictions or requirements may apply. Please contact our customer\n",
            "\n",
            "support team for assistance with international returns.\n",
            "\n",
            "2.7 Multiple Returns:\n",
            "\n",
            "Monitoring Returns: AcmeCorp monitors returns activity for abuse of the policy.\n",
            "\n",
            "Excessive returns may result in denial of future return requests or account suspension.\n",
            "\n",
            "Chapter 3: Return Process\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and Using the FAISS Index for Retrieval\n",
        "\n",
        "In this step, we load the previously saved FAISS index and use it to perform similarity searches. This is a crucial part of the Retrieval-Augmented Generation (RAG) workflow, as it allows us to efficiently find relevant chunks of information in response to a query.\n",
        "\n",
        "#### Why Load the Index?\n",
        "The FAISS index is a precomputed data structure that organizes embeddings in a way that makes similarity searches fast and scalable. By saving and reloading the index:\n",
        "- **Persistence:** We avoid recomputing embeddings and re-indexing documents every time the program runs.\n",
        "- **Scalability:** The index can be stored and distributed across systems, enabling large-scale applications.\n",
        "- **Efficiency:** Loading the index allows immediate access to the vector database for retrieval tasks.\n",
        "\n",
        "#### The Code Breakdown\n",
        "```python\n",
        "# Load the FAISS index from the saved location\n",
        "db = FAISS.load_local(\n",
        "    folder_path=\"../database/faiss_db\",  # Path to the saved FAISS index\n",
        "    embeddings=embedding,               # The embedding model used to create the index\n",
        "    index_name=\"myFaissIndex\"           # The specific name of the index to load\n",
        ")\n",
        "```\n",
        "`folder_path`: Specifies the directory where the FAISS index was saved.\n",
        "\n",
        "`embeddings`: Ensures that the same embedding model used to create the index is loaded for accurate similarity searches.\n",
        "\n",
        "`index_name`: Names the index for easy identification, especially when managing multiple indices.\n",
        "\n",
        "#####Performing a Similarity Search\n",
        "```python\n",
        "searchDocs = db.similarity_search(\"What is the return policy for?\")\n",
        "print(searchDocs[0].page_content)\n",
        "```\n",
        "\n",
        "`similarity_search`: Takes a query string as input and searches the index for the most similar chunks of text.\n",
        "\n",
        "`Query Example`: In this case, we are searching for information about a return policy.\n",
        "\n",
        "`Result`: The method retrieves the most relevant chunks and returns them in descending order of similarity. Here, we print the content of the top result.\n",
        "\n",
        "#####Key Benefits of FAISS for RAG\n",
        "\n",
        "*Speed*: FAISS is optimized for fast nearest-neighbor searches, even with large datasets.\n",
        "\n",
        "*Scalability*: It supports billions of vectors, making it ideal for large-scale document retrieval.\n",
        "\n",
        "*Flexibility*: The loaded index can be queried multiple times with different inputs, enabling dynamic interactions.\n",
        "\n",
        "By completing this, you enable the core functionality of RAG: retrieving relevant context to enhance the output quality of downstream tasks, such as generating precise and accurate responses."
      ],
      "metadata": {
        "id": "MFNQ0XX7jPTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Augumenting the content above into an LLM query**\n",
        "\n",
        "Now that we have retrieved the most relevant content from the file we imported our data from, let us query the document using an LLM to generate a nice response.\n",
        "\n",
        "We will be using the `llama3-8b-8192` LLM to generate our reply. The LLM is hosted on [Groqcloud](https://console.groqcloud.com). Once you create an account with Groqcloud create an API key and set that in order to use the code below to send a request to the LLM and get a response back\n",
        "\n",
        "### Setting Up Your API Key in Google Colab\n",
        "\n",
        "To securely set your API key or secret in this notebook:\n",
        "1. Run the provided code cell that prompts for the API key.\n",
        "2. Enter your API key when prompted. The input will be hidden for security.\n",
        "3. The key will be stored as an environment variable for use in subsequent code.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZMEi7CJRj8kR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass(\"Enter your API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaTnA3Hnl3Dm",
        "outputId": "d9ac9b89-2723-481c-cf27-047c30422133"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have set your API key, let us import the required libary for Groq and run the code against the libary\n"
      ],
      "metadata": {
        "id": "a_mYCQvem6pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install groq\n",
        "from groq import Groq"
      ],
      "metadata": {
        "id": "DyaqwyyRnC-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let us get the context related content again by loading the index and specifying the question"
      ],
      "metadata": {
        "id": "0AnKrgZhnddY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the refund policy?\"\n",
        "\n",
        "db = FAISS.load_local(folder_path=\"../database/faiss_db\", embeddings=embedding, index_name=\"myFaissIndex\", allow_dangerous_deserialization=True)\n",
        "searchDocs = db.similarity_search(question)\n",
        "\n",
        "answer = searchDocs[0].page_content\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpaGCTRrnbcj",
        "outputId": "25e037ba-b949-4f96-d860-22fbe333078f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promotional Items: Items purchased during promotions or sales may be subject to\n",
            "\n",
            "special return or refund conditions. Please refer to the terms of the promotion for\n",
            "\n",
            "specific details.\n",
            "\n",
            "4.7 Refund or Exchange Denial:\n",
            "\n",
            "Non-Eligible Returns: Items that do not meet the eligibility criteria for returns (as\n",
            "\n",
            "outlined in Chapter 2) will not be refunded or exchanged. You will be notified of the\n",
            "\n",
            "reason for denial, and the item may be returned to you at your expense.\n",
            "\n",
            "Policy Abuse: AcmeCorp reserves the right to deny refunds or exchanges in cases of\n",
            "\n",
            "policy abuse or fraudulent activity. Excessive return requests may be flagged and\n",
            "\n",
            "result in account suspension or denial of future returns.\n",
            "\n",
            "4.8 Customer Support:\n",
            "\n",
            "Assistance: For any questions or assistance with refunds or exchanges, please contact\n",
            "\n",
            "our customer support team. We are here to help and ensure your experience with\n",
            "\n",
            "AcmeCorp is positive.\n",
            "\n",
            "By understanding and following these guidelines, you can ensure a smooth refund or\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "client = Groq()\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"llama3-8b-8192\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Play the role of a customer service professional \"\n",
        "            \"for a leading E-Commerce company called AcmeCorp and \"\n",
        "            \"analyse a provided question or ticket from a customer. \"\n",
        "            \"Be as professional and polite as possible when replying \"\n",
        "            \"to the provided question. \"\n",
        "            \"The response  a good response will be based on the information \"\n",
        "            \"provided here only. Do not include information not in the follwing\"\n",
        "            \"context: \" + answer\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": question\n",
        "        }\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    stream=True,\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "for chunk in completion:\n",
        "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4Lfgl79lLPh",
        "outputId": "dc87a2ae-c8c4-4b04-8f91-87a48a698b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for reaching out to AcmeCorp's customer support team! We're happy to help you understand our refund policy.\n",
            "\n",
            "At AcmeCorp, we strive to provide a hassle-free shopping experience for our customers. Our refund policy is designed to ensure that you're satisfied with your purchase. Here are the key points to note:\n",
            "\n",
            "* Eligible returns: You can return or exchange an item within [insert timeframe] of receiving your order, as long as the item is in its original condition with all original tags and packaging intact.\n",
            "* Non-eligible returns: Items that do not meet our eligibility criteria for returns, such as items that are worn, damaged, or missing original tags and packaging, will not be refunded or exchanged.\n",
            "* Refund or exchange denial: If your return is denied, you will be notified of the reason for denial, and the item may be returned to you at your expense.\n",
            "* Policy abuse: We reserve the right to deny refunds or exchanges in cases of policy abuse or fraudulent activity. Excessive return requests may be flagged and result in account suspension or denial of future returns.\n",
            "\n",
            "If you have any questions or concerns about our refund policy, please don't hesitate to reach out to our customer support team. We're here to help and ensure your experience with"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Explanation\n",
        "\n",
        "#### Overview\n",
        "This code interacts with the **Groq API** to generate a response from a Large Language Model (LLM) in a conversational setup. It simulates a customer service assistant for the fictional company **AcmeCorp**, responding to user queries based on a predefined context.\n",
        "\n",
        "---\n",
        "\n",
        "#### Key Components and Explanation\n",
        "\n",
        "1. **Creating the Groq Client**\n",
        "   ```python\n",
        "   client = Groq()\n",
        "```\n",
        "2. **Calling the Chat Completion Endpoint**\n",
        "\n",
        "```python\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"llama3-8b-8192\",\n",
        "    ...\n",
        ")\n",
        "```\n",
        "\n",
        "**Purpose**:\n",
        "This sends a request to Groq's chat.completions.create endpoint to generate a conversational response.\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "`model`: Specifies the LLM to use. Here, the model is llama3-8b-8192, which likely has 8 billion parameters and supports an 8192-token context window.\n",
        "\n",
        "`messages`: A list of messages simulating a chat conversation. Each message includes:\n",
        "- role: The role of the message sender (system, user).\n",
        "- content: The message content.\n",
        "\n",
        "**Other Arguments**:\n",
        "- `temperature=0`: Ensures deterministic output by removing randomness. The lower the value, the more focused the responses.\n",
        "\n",
        "- `max_tokens=256`: Limits the response length to 256 tokens.\n",
        "\n",
        "- `top_p=1`: Implements nucleus sampling. With 1, all token probabilities are considered, ensuring completeness.\n",
        "- `stream=True`: Enables streaming, where the response is sent in chunks.\n",
        "\n",
        "3. **Message Structure**\n",
        "- `system` role: Provides instructions to the LLM on how to behave and limits its responses to the given answer context. The variable answer contains relevant context for the query.\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"Play the role of a customer service professional for a leading E-Commerce company called AcmeCorp ... context: \" + answer\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "- `user` role:\n",
        "Represents the user's question, stored in the variable question.\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": question\n",
        "}\n",
        "```\n",
        "\n",
        "4. **Streaming and Printing the Response**\n",
        "\n",
        "```python\n",
        "for chunk in completion:\n",
        "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
        "```\n",
        "\n",
        "- `Streaming`: Iterates over chunks of the model's response as they arrive in real time.\n",
        "- Accessing Content:\n",
        "`chunk.choices[0].delta.content`: Extracts the text from the response chunk.\n",
        "or \"\": Ensures no interruptions if a chunk is empty.\n",
        "\n",
        "- `Output`: Prints the response continuously as it is generated, giving the user a real-time experience."
      ],
      "metadata": {
        "id": "MqCq3wKmo_j-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4JnQfZWoW4J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}